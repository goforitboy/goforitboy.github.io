---
title: ai agent+vla+rl+slam 
date: 2026/1/28   # 文章发表时间
categories: Tech # 分类
thumbnail: /images/ai_agent+vla+rl+slam背景.jpg # 略缩图
---
这一晃又5天没写了，这几天主要是和高中同学好好聚了一聚，所以这些东西也是闲的没事的时候用零散的时间学一学，但是毕竟是5天的时间，所以积少成多也了解了不少的东西，简单写写吧。

## AI Agent实战
这一部分我花了1-2天的时间实践了一下，用cursor构建了一个学习知识的agent: 你输入要学习的东西（比如transformer）,然后他会给你告诉你都有哪些学习内容，和学习路线，而且你还可以再把你的时间表给他发过去，他会根据你的空闲时间去为你安排每天的学习时间。（这个编程大模型现在实在是强，多强自己去试一下就知道了）

<div style="text-align: center;">
  <img src="/images/ustudy.png"  style="width: 700px;" />
</div>

不过因为我这次本身也不是冲着完整的搭建一个学习软件去的，所以我这里只使用Opus4.5把基础框架和交互功能给搞了出来，能做出基础的学习内容和框架，但因为没继续优化，所以回答的也是一坨，而且没有一个成型的UI。而且还有一个原因，就是我就构建了一个基础框架，就花了6刀的额度，然后我再简单优化两下再试一下RAG，就到9刀了，这玩意强是真强，吞钱也是真吞钱。。

然后我还简单试了一下分层的RAG（检索增强生成），理论上是可以自己添加学习资料了，然后比如你想学习某一部分的内容，llm会在你提供的资料里进行分层级的检索，然后把你想学的东西提取并整合，再发给你，那么分层级具体体现在哪里呢，可以看下面这张图。

<div style="text-align: center;">
  <img src="/images/分层级的RAG.png"  style="width: 700px;" />
</div>

当然了，这个所谓的分层级也就是我随便试了一试，所以功能肯定还不够完善，主要是通过这种方式来了解一下agent以及RAG吧。

## VLA（模仿学习）
完整的看完了[同济子豪兄的**lerobot**教程](https://zihao-ai.feishu.cn/wiki/TS6swApHbinx01kHDi5cf5n5n8c)，主要是在模仿学习这一领域的应用，模仿学习就是人类拖拽机器人示教采集数据集，形成数据集。然后用这个数据集训练模仿学习算法，最终部署在机器人上，让机器人自主模仿人类动作，泛化到真实环境。无需遥操作和遥控。

硬件没啥好说的，控制舵机那里和上个学期用上位机控制关节转动几乎一样，但是现在我还没接触到硬件，不知道实际调试的时候会不会有什么奇怪的问题。

然后就了解了一下模仿学习里的几个核心策略，single_task policy主要就是**ACT**，特点是轻量化，上手快，episode也不需要很多，50个就够（尽管最后效果没有smolvla好），general policies主要是**smolvla , pi0 , pi0.5**这三个策略。

最后把官方教程里ACT策略的论文讲解给看完了，这篇论文核心点在于VAE变分自编码器，动作分块（action chunking）这两个点。
### ACT（Action Chunking with Transformers）
ACT是 一种高级模仿学习策略，它不是一步一步预测动作，而是：
1.预测一段连续的动作序列（chunk）
2.同时使用 Transformer 结构对时间序列建模
3.有助于降低模仿学习中的误差累积问题

但要让 Transformer 策略学得好，仅仅预测动作序列本身还不够，它还要处理以下现实问题：
1.人类演示行为本身具有很大的变化性（不同人、不同策略、不同速度）
2.训练数据不是“完美确定的”
3.需要学习“除了视觉和状态之外的风格差异”（也就是人一般会怎么做，这里就体现出了“模仿”学习）
这正是 VAE 能发挥作用的地方。

VAE 在 ACT 中的具体作用：
把不同演示中相同任务但不同风格的动作序列，通过transformer encoder（编码器）压缩成一个可以“分布式编码”的潜变量，然后让 transformer decoder（解码器） 在这个潜变量控制下预测动作序列。

通俗一点的解释：
三个专家都完成同一个任务，但动作细节不一样。
VAE 的优势在于：
它不是把每条演示当成一个固定样例，而是把它们“看成是一种分布”，让模型学会：“这个任务大致是什么样的动作模式”，加上“这一条演示的个性化变化”

在 ACT 的训练框架里，VAE（特别是 CVAE）有两个关键角色：
1.编码“风格变量”（encoder）
对于每个动作序列（chunk），VAE 编码器会输出： 
（1）**潜变量分布**（μ mean + σ variance）
（2）从这个分布中采样一个 **latent z**
这个 z 不是最终动作，它是 “对这段演示的紧凑表达”。
2.作为 Transformer 预测动作序列的条件
在 Transformer 做动作预测时，它不仅输入：
（1）当前视觉状态
（2）当前机器人关节
（3）未来动作时间轴
还输入了：
**这个 VAE 生成的 latent z**

这意味着：
VAE 帮助策略把“风格或上下文信息”带进到 Transformer 的序列预测里，**可以理解为这个潜变量分布或者这个z就是人类专家去做这件事的风格，会怎么去做这件事**。

用一句话总结：在 ACT 里，VAE（通常是条件 VAE）不是用来直接“生成动作”，而是用来提炼和压缩演示数据中隐藏的风格和多样性，再把这种风格条件传递给 Transformer，让策略生成的动作更连贯、更鲁棒、更符合人类演示的规律。

## RL（强化学习）
这里我主要了解了HIL-SERL这一RL策略，简单整理一下：
强化学习（Reinforcement Learning, RL）是让智能体通过与环境反复交互、自主试错来学习策略的过程。与传统监督学习需要大量标注数据不同，RL 让智能体自己探索，从奖励信号中学会什么是“好行为”——适用于机器人控制这类决策问题。

而 **HIL-SERL 是 RL 的一个具体变种**，它特别适合真实机器人学习，因为它结合了：
1.**人类演示**（Human Demonstrations）
2.**在线学习**（Online Reinforcement Learning）
3.**人在环路干预**（Human-in-the-Loop Interventions）
这样做的好处是：**既有来自人类专家的初始经验，又有 RL 的探索能力，还能在学习时人为纠正危险或无效行为，从而大幅提升效率与安全性。**

流程图如下：
```
准备环境
  ↓
人类示范数据收集
  ↓
训练奖励分类器（模仿学习起点）
  ↓
RL 策略训练循环
  ↳模型尝试动作 → 获得奖励信号（分类器结果）
  ↳人类随时干预（修正）
  ↳学习器更新策略
循环直到策略成功
  ↓
评估 / 部署策略
```
## Slam
SLAM = Robot 在未知环境中，一边“自己找路定位自己在哪里”（定位），一边“建立环境地图”（建图）。
Slam全称是Simultaneous Localization And Mapping（同时定位与地图构建）

核心循环流程如下：
```
传感器数据（视觉/激光/IMU）
↓
特征提取
↓
特征匹配（过去 vs 现在）
↓
里程计预测位置（先验）
↓
地图更新（把新信息加到地图）
↓
位置估计优化（修正）
↓
下一个循环
```
像slam这种东西，就得实战，基础知识其实就那点东西，大致知道是干啥的就足够了。
## 注意力机制
简单了解了**注意力机制，自注意力机制，交叉注意力机制**：
注意力机制是让模型在处理信息时“重点关注最有用的部分”；
自注意力机制是让每个词都对自己序列中的其他词“看一遍、算重要性”，然后做出更好的表达；
交叉注意力机制是让一个序列去关注另一个序列，对它们之间的对应关系评分。

通俗点说：
自注意力就是一个人说话时，他自己先理清自己说的每一句内容之间的关系。（谁是谁？话题是什么？上下文怎么串起来？）
交叉注意力就是 听 A 说话（源语），然后你用自己的语言（目标语）来表达、解释、翻译时，不停地对照 A 说的每一句关键内容。

**补充：step,eposide,batch,epoch的区别**
1.step在IL（模仿学习）中指的是训练过程中的step（梯度参数更新一次），在RL中指的是环境交互的step，也就是那一个小循环。
2.episode在模仿学习中指的是比如在抓取任务里从机械臂初始状态开始，到成功抓取立方体或时间用完结束。**一个 episode 由很多 steps 组成。**
3.batch指的是训练一次模型使用的样本数量，模型更新一次权重参数的过程，通常对应处理一个batch。
4.epoch指的是训练时数据集被完整遍历一次。