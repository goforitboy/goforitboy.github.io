---
title: Agentic AI 
date: 2026/1/19   # 文章发表时间
categories: Tech # 分类
thumbnail: /images/Agentic_AI背景.jpg # 略缩图
---
这一天半肝完了Andrew Ng的Agentic AI,说是肝，但是总感觉效率还是不高，在家里手机放在旁边还是时不时会看两眼，虽然本身就对这些东西很感兴趣，但是听到没有意思的地方还是会去想干点其他有意思的事，这一点还是需要改。

简单总结一下这门课程吧，由于[github](https://github.com/datawhalechina/agentic-ai.git)和[CSDN](https://blog.csdn.net/cfy2401926342?type=blog)上都已经有大佬总结出非常详细的笔记了，所以我就简单记录一下对于每个模块的总结和感受。

# 一、Agentic工作流简介
首先Andrew提出了很有意思的一个点，那就是我们平时经常提到AI agent,但是这里他的课程标题是Agentic AI,那么这一点主要是因为现在有很多人不停的争论一个问题，就是根据一个系统的自主性，他到底属不属于ai agent,但是Andrew认为agent的自主性本身就是具备高低之分的，所以争论这个问题完全没必要，因此就直接把agent这个词变成一个形容词agentic来放在ai的前面，这样就可以巧妙的将这个问题解决了，在这里也能看出作为一个横跨学术界和工业界的大亨的严谨性了。

那么agentic ai到底是个什么东西呢，其实说白了，就是相当于你雇了一个助手，这个助手啊他很聪明，你告诉他我要干什么以及我的要求，我能提供的材料，然后他能自己规划达成这个目标的路线，包括在这个过程中还要使用一些工具，比如进行网页抓取，下载文献等，最后自动完成这一系列的流程，且完成效果很好。那说到这里，就不得不提一下这个agentic ai到底比我们平时用的大语言模型强在哪里呢？

第一点有些东西大语言模型他做不了但是agent能做，可能你让大语言模型写一个文章没问题，问他点常识他能给你讲明白，但是如果你要实现一个复杂的流程那就根本不在他的能力范围内，所以这里就需要agent来去应用一些工具，来去接收多方面多渠道的信息，以及对接一些软件的api去端到端的完成一些任务，那么agent就会表现的更好，比如完成一套完整的从市场调研，方案设计，产品设计，写出营销文案的流程，单单一个大语言模型显然力不从心。

第二点是大语言模型是一次性生成的，所以他不会来回反复修改，也不会自己自动评估，只能靠我们人类自己评估，因此很费时间，也不准确，但是agent里面很重要的部分就是反思和评估，所以他可以在很大程度上提升文章的专业度，以及任务的流畅度和完整度。所以如果你用非智能体写个大作业可能还行，但是你要是要写一个领域的调研报告或者论文，那ai agent无疑是更好的选择。

第三点其实我觉得就是agent工作流的本质了，可能我们平时用gpt时也能感觉到，对于从头到尾的一个项目构建过程，可能只有一部分比如写代码可以用大语言模型来完成，然而对于比如调整代码之间的关系以及喂数据，上线部署等任务都要我们人工去完成，这些东西本身就是流程化的东西，那就没有必要我们人类总是重复去做，因此这个agent工作流的重要性想必就不言而喻了。可以想象一下，在几年之后，大多数机械性的工作，都可以由一个agent去代替你完成，你只需要在你的prompt（甚至都已经为你写好模版）中填上一些数据，然后一个回车，你就可以去睡觉了，需要完成的步骤agent会自主规划好，需要获取的信息会在网上查的明明白白，一觉醒来这个项目就完成了。在人类休息的时候还能自主的去工作，甚至自己知道自己下一步应该干什么，我想这个也就是agentic ai的意义所在了。

至于几个案例，我就不详细解释了，笔记真的写的很清楚很清楚了，真的感谢能整理出这么详细的笔记的大佬。

那么简单说说ai agent中的几个核心的设计模式，分别是**反思**，**工具使用**和**规划**。

# 二、反思设计模式
人类如果写一篇文章或者写一段代码之后，会返回去检查是不是有拼写错误，有没有哪些遗漏的地方，我们可以把这一反思过程也让ai agent去自主的完成。

案例：代码编写（这里会借用笔记里的部分内容，写的很清晰）

1.基础版：模型自省
步骤1： 提示 LLM 编写一段代码（code V1）。
步骤2： 将 code V1 再次输入给 LLM，提示其“检查代码中的错误并写出改进版”。
结果： 得到修复了潜在 bug 的 code V2。

2.进阶版：利用“思考模型” (Reasoning Models)去“检查代码中的错误并写出改进版”
不同的 LLM 有不同的专长。可以使用一个擅长快速生成的模型来写初稿，再用一个“思考模型”（Thinking Model）、更擅长逻辑推理和错误排查的模型来进行反思和改进。
这种组合能发挥各自的优势，达到“1+1>2”的效果。

3.终极形态：结合外部反馈的反思
反思最强大的形式是结合外部反馈。仅仅让模型“内省”是有限的，而引入来自模型之外的新信息，则能带来质的飞跃。

以上面的代码为例做结合外部反馈的反思：
1.生成初稿： LLM 生成 code V1。
2.执行代码： 在安全的沙盒环境中运行 code V1。
3.获取外部反馈： 系统捕获代码的实际输出（Output）和任何错误信息（Errors），例如 SyntaxError: unterminated string literal。
4.反思与改进： 将 code V1、Output 和 Errors 一起作为输入，交给 LLM 进行反思，要求其根据这些具体的、客观的反馈来重写代码。
5.获得终稿： LLM 基于真实的执行结果，修正错误，生成功能正确的 code V2。

Andrew在这里还特意提到了在agent的反思过程中，如果能有来自模型之外的新信息输入，那将会带来非常大的性能提升。所以也就明白为什么叫反思模式下的终极形态了。具体的例子可以参考一下图表生成工作流，这个例子看完就能彻底明白了。

而对于反思并调整之后的效果是怎么评估的呢，有两种评估模式：
1.客观评估：
构建带“真实答案”的数据集，用代码自动计算正确率。
简单、易管理、结果客观。适用于有明确答案的任务（如数据库查询）。
2.主观评估：
使用 LLM 作为裁判，但是由于评判标准不同，最终结果会有偏差，因此需提供详细的评分量表（Rubric）。
需要更多调优，但能处理复杂的主观标准（如图表美观度）。

这里还有个有意思的点，可以称之为位置偏见（Position bias），即我们在让模型在两者中选其一时，许多模型倾向于选择第一个输入的选项（A），无论其质量如何。

在这一模块的最后呢，Andrew也用前面的三个例子更好的解释了一下外部反馈：


| Challenge (挑战) | Example (示例) | Source of feedback (反馈来源) |
| :--- | :--- | :--- |
| **提及竞争对手** | “我们公司的鞋子比 RivalCo 好” | **模式匹配** (Pattern matching for competitor names)<br> *使用正则表达式等工具扫描输出，若发现竞争对手名字，则将其作为批评性输入反馈给模型，要求其重写文本。* |
| **事实核查文章** | “泰姬陵建于1648年” | **网络搜索结果** (Web search results)<br> *通过网络搜索核实历史事实（如泰姬陵实际于1631年下令建造，1648年完工），并将精确的时间段作为额外输入提供给反思智能体，以生成更准确的版本。* |
| **超出字数限制** | 生成的文章超过指定字数 | **字数统计工具** (Word count tool)<br> *编写代码精确统计字数，如果超出限制，则将该信息反馈给 LLM，要求其重新尝试，以更准确地达到期望的输出长度。* |

# 三、工具使用

（未完待续）